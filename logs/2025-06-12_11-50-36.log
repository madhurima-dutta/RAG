[2025-06-12 11:50:39] - INFO - app - User question: explain me how the DistilRoBERTa works?
[2025-06-12 11:50:39] - INFO - app - Conversational chain created using gemini-1.5-flash
[2025-06-12 11:50:45] - INFO - app - Response generated: DistilRoBERTa is a smaller, faster version of RoBERTa, retaining 97% of RoBERTa's language understanding while being 60% faster and using 40% fewer parameters.  It excels at capturing contextual nuances in conversations, making it ideal for dialogue-based emotion classification.  Pre-trained on a vast amount of normal text data, it's highly adaptable for fine-tuning with emotion-specific datasets.  In this system, a fine-tuned DistilRoBERTa model, trained on dialogue transcripts from the *Friends* series, classifies 9 Ekman emotions (Delightful, Joy, Happy, Surprised, neutral, Sadness, Fear, Anger, Disgust) plus a neutral category.  The model output is a softmax probability across these 10 categories, representing the approximate emotion.
